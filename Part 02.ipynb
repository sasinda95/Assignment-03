{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, grid_size):\n",
    "        self.n_row = grid_size\n",
    "        self.n_col = grid_size\n",
    "        self.n_state = self.n_row * self.n_col\n",
    "        self.n_action = 4\n",
    "\n",
    "        # Define actions\n",
    "        self.action = [[0, -1], [1, 0], [0, 1], [-1, 0]]\n",
    "        self.action_text = ['←', '↓', '→', '↑']\n",
    "\n",
    "        # Define initial state\n",
    "        self.state_0 = 3 * self.n_row + 3  # Center of the grid (3, 3)\n",
    "\n",
    "        # Define terminal states\n",
    "        self.terminal_states = {6 * self.n_row + 0: -1, 0 * self.n_row + 6: 1}\n",
    "\n",
    "        # Define model\n",
    "        self.model = [[[] for _ in range(self.n_action)] for _ in range(self.n_state)]\n",
    "        for s in range(self.n_state):\n",
    "            for a in range(self.n_action):\n",
    "                row, col = np.divmod(s, self.n_row)\n",
    "                act = self.action[a]\n",
    "                row_, col_ = row + act[0], col + act[1]\n",
    "                state_ = row_ * self.n_row + col_\n",
    "                outsidecheck = (row_ < 0) or (col_ < 0) or (row_ >= self.n_row) or (col_ >= self.n_col)\n",
    "\n",
    "                if outsidecheck:\n",
    "                    self.model[s][a].append([1.0, s, 0.0, False])  # if trying to move outside the grid\n",
    "                elif state_ in self.terminal_states:\n",
    "                    self.model[s][a].append([1.0, state_, self.terminal_states[state_], True])  # if moving to a terminal state\n",
    "                else:\n",
    "                    self.model[s][a].append([1.0, state_, 0.0, False])  # normal movement\n",
    "\n",
    "# Function to encode the state using one-hot encoding\n",
    "def Affine(state, env):\n",
    "    fv = np.zeros(env.n_state)\n",
    "    fv[state] = 1\n",
    "    return fv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the exact value function\n",
    "def compute_exact_value_function(env, policy, gamma):\n",
    "    P = np.zeros((env.n_state, env.n_state))\n",
    "    R = np.zeros(env.n_state)\n",
    "\n",
    "    for s in range(env.n_state):\n",
    "        if s in env.terminal_states:\n",
    "            continue\n",
    "        for a in range(env.n_action):\n",
    "            if policy[s][a] > 0:  # Only consider actions with non-zero probability\n",
    "                for p, s_, r, t in env.model[s][a]:\n",
    "                    P[s, s_] += policy[s][a] * p\n",
    "                    R[s] += policy[s][a] * p * r\n",
    "\n",
    "    I = np.eye(env.n_state)\n",
    "    V = np.linalg.solve(I - gamma * P, R)\n",
    "\n",
    "    # Set terminal states to zero\n",
    "    for state in env.terminal_states:\n",
    "        V[state] = 0\n",
    "\n",
    "    return np.round(V, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Monte Carlo method\n",
    "def Gradient_Monte_Carlo(max_ep, alpha, gamma, policy, env):\n",
    "    w = np.zeros(env.n_state)\n",
    "\n",
    "    for ep in range(max_ep):\n",
    "        s = env.state_0\n",
    "        Traces = []\n",
    "\n",
    "        # Generate an episode\n",
    "        while True:\n",
    "            a = np.random.choice(np.arange(env.n_action), p=policy[s])\n",
    "            _, s_, r, t = env.model[s][a][0]\n",
    "\n",
    "            Traces.append([s, a, r])\n",
    "            s = s_\n",
    "\n",
    "            if t:\n",
    "                break\n",
    "\n",
    "        G = 0\n",
    "        for t, trace in enumerate(Traces[::-1]):\n",
    "            G = gamma * G + trace[2]\n",
    "            x = Affine(trace[0], env)\n",
    "            if trace[0] not in env.terminal_states:\n",
    "                w += alpha * (G - np.dot(w, x)) * x\n",
    "\n",
    "    return np.round(w, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi-Gradient TD(0) method\n",
    "def Semi_Gradient_TD0(max_ep, alpha, gamma, policy, env):\n",
    "    w = np.zeros(env.n_state)\n",
    "\n",
    "    for ep in range(max_ep):\n",
    "        s = env.state_0\n",
    "\n",
    "        while True:\n",
    "            a = np.random.choice(np.arange(env.n_action), p=policy[s])\n",
    "            _, s_, r, t = env.model[s][a][0]\n",
    "\n",
    "            x = Affine(s, env)\n",
    "            x_ = Affine(s_, env)\n",
    "            if s not in env.terminal_states:\n",
    "                w += alpha * (r + gamma * np.dot(w, x_) - np.dot(w, x)) * x\n",
    "\n",
    "            s = s_\n",
    "\n",
    "            if t:\n",
    "                break\n",
    "\n",
    "    return np.round(w, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the optimal policy based on the value function\n",
    "def find_policy(gamma, V, env):\n",
    "    policy_opt = np.zeros((env.n_state, env.n_action))\n",
    "    for s in range(env.n_state):\n",
    "        q_values = []\n",
    "        for a in range(env.n_action):\n",
    "            q_value = 0\n",
    "            for p, s_, r, t in env.model[s][a]:\n",
    "                q_value += p * (r + gamma * np.dot(V, Affine(s_, env)))\n",
    "            q_values.append(q_value)\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy_opt[s] = np.eye(env.n_action)[best_action]\n",
    "    return policy_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and initialize the environment\n",
    "env = Environment(grid_size=7)\n",
    "\n",
    "# Define the initial random policy\n",
    "policy = np.ones((env.n_state, env.n_action)) / env.n_action\n",
    "\n",
    "# Compute the exact value function\n",
    "gamma = 0.9\n",
    "V_exact = compute_exact_value_function(env, policy, gamma)\n",
    "\n",
    "# Perform Gradient Monte Carlo\n",
    "alpha = 0.01\n",
    "max_ep = 10000\n",
    "VF_gmc = Gradient_Monte_Carlo(max_ep, alpha, gamma, policy, env)\n",
    "\n",
    "# Perform Semi-Gradient TD(0)\n",
    "VF_td0 = Semi_Gradient_TD0(max_ep, alpha, gamma, policy, env)\n",
    "\n",
    "# Reshape the value functions to match the grid layout\n",
    "V_exact_grid = V_exact.reshape((env.n_row, env.n_col))\n",
    "VF_gmc_grid = VF_gmc.reshape((env.n_row, env.n_col))\n",
    "VF_td0_grid = VF_td0.reshape((env.n_row, env.n_col))\n",
    "\n",
    "# Function to determine the optimal policy based on the value function\n",
    "def get_optimal_policy_from_value(V_grid, env):\n",
    "    optimal_policy = np.full((env.n_row, env.n_col), 'x', dtype='<U2')\n",
    "    for i in range(env.n_row):\n",
    "        for j in range(env.n_col):\n",
    "            state = i * env.n_row + j\n",
    "            if state not in env.terminal_states:\n",
    "                q_values = []\n",
    "                for a in range(env.n_action):\n",
    "                    q_value = 0\n",
    "                    for p, s_, r, t in env.model[state][a]:\n",
    "                        q_value += p * (r + gamma * V_grid[s_ // env.n_row, s_ % env.n_col])\n",
    "                    q_values.append(q_value)\n",
    "                best_action = np.argmax(q_values)\n",
    "                optimal_policy[i, j] = env.action_text[best_action]\n",
    "    return optimal_policy\n",
    "\n",
    "optimal_policy_exact = get_optimal_policy_from_value(V_exact_grid, env)\n",
    "optimal_policy_gmc = get_optimal_policy_from_value(VF_gmc_grid, env)\n",
    "optimal_policy_td0 = get_optimal_policy_from_value(VF_td0_grid, env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Value Function:\n",
      " 0.00  0.01  0.03  0.08  0.20  0.46  0.00\n",
      "-0.01  0.00  0.02  0.06  0.13  0.27  0.46\n",
      "-0.03 -0.02  0.00  0.03  0.07  0.13  0.20\n",
      "-0.08 -0.06 -0.03  0.00  0.03  0.06  0.08\n",
      "-0.20 -0.13 -0.07 -0.03  0.00  0.02  0.03\n",
      "-0.46 -0.27 -0.13 -0.06 -0.02  0.00  0.01\n",
      " 0.00 -0.46 -0.20 -0.08 -0.03 -0.01  0.00\n",
      "\n",
      "Optimal Policy:\n",
      "→ → → → → → x\n",
      "→ → → → → → ↑\n",
      "↑ → → → → ↑ ↑\n",
      "↑ ↑ → → ↑ ↑ ↑\n",
      "↑ ↑ → → ↑ ↑ ↑\n",
      "↑ → → → → ↑ ↑\n",
      "x → → → → → ↑\n",
      "\n",
      "\n",
      "Gradient Monte Carlo Value Function:\n",
      "-0.00  0.01  0.05  0.12  0.24  0.52  0.00\n",
      "-0.01  0.01  0.03  0.09  0.19  0.33  0.46\n",
      "-0.04 -0.03  0.00  0.06  0.09  0.16  0.17\n",
      "-0.10 -0.07 -0.02  0.00  0.02  0.07  0.07\n",
      "-0.23 -0.15 -0.06 -0.04 -0.01  0.04  0.02\n",
      "-0.45 -0.28 -0.15 -0.10 -0.06 -0.00 -0.01\n",
      " 0.00 -0.46 -0.20 -0.10 -0.06 -0.03  0.00\n",
      "\n",
      "Optimal Policy:\n",
      "→ → → → → → x\n",
      "→ → → → → ↑ ↑\n",
      "↑ ↑ → → ↑ ↑ ↑\n",
      "↑ → → ↑ ↑ ↑ ↑\n",
      "↑ → ↑ ↑ → ↑ ↑\n",
      "↑ → ↑ ↑ → ↑ ↑\n",
      "x → → → → → ↓\n",
      "\n",
      "\n",
      "Semi-Gradient TD(0) Value Function:\n",
      " 0.00  0.01  0.03  0.07  0.20  0.50  0.00\n",
      "-0.01  0.00  0.02  0.06  0.12  0.26  0.47\n",
      "-0.03 -0.02  0.00  0.03  0.08  0.15  0.20\n",
      "-0.08 -0.06 -0.03  0.01  0.03  0.06  0.09\n",
      "-0.21 -0.12 -0.07 -0.03 -0.00  0.02  0.04\n",
      "-0.49 -0.26 -0.13 -0.06 -0.02  0.00  0.01\n",
      " 0.00 -0.42 -0.17 -0.08 -0.03 -0.01  0.00\n",
      "\n",
      "Optimal Policy:\n",
      "→ → → → → → x\n",
      "→ → → → → ↑ ↑\n",
      "↑ → → → → ↑ ↑\n",
      "↑ ↑ → → ↑ ↑ ↑\n",
      "↑ ↑ → ↑ ↑ ↑ ↑\n",
      "↑ ↑ → → → ↑ ↑\n",
      "x → → → → → ↑\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the value functions and optimal policies\n",
    "def print_value_function_and_policy(V_grid, policy, title):\n",
    "    print(f\"{title} Value Function:\")\n",
    "    for row in V_grid:\n",
    "        print(\" \".join(f\"{val:5.2f}\" for val in row))\n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    for row in policy:\n",
    "        print(\" \".join(row))\n",
    "    print(\"\\n\")\n",
    "\n",
    "print_value_function_and_policy(V_exact_grid, optimal_policy_exact, \"Exact\")\n",
    "print_value_function_and_policy(VF_gmc_grid, optimal_policy_gmc, \"Gradient Monte Carlo\")\n",
    "print_value_function_and_policy(VF_td0_grid, optimal_policy_td0, \"Semi-Gradient TD(0)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sasinda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
